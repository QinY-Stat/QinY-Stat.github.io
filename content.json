{"meta":{"title":"Qin","subtitle":"People Have Dreams","description":"A Master Candidate in Statistics, NLP/Social Network/Data Mining.","author":"Qin Yue","url":"http://github.com/QinY-Stat","root":"/"},"pages":[{"title":"关于我","date":"2020-02-08T10:03:55.000Z","updated":"2020-02-08T11:25:50.093Z","comments":true,"path":"about/index.html","permalink":"http://github.com/QinY-Stat/about/index.html","excerpt":"","text":"一个吃过数学苦、流过统计泪的即将毕业的硕士, 研究兴趣包括机器学习、数据挖掘、社会网络、NLP 姓名：覃悦家乡：柳州现居：天津Github：QinY-Stat知乎：覃字念Qin不念Tan微博：覃字念Qin不念Tan微信：qy362019599QQ：362019599"}],"posts":[{"title":"Tensorflow加速Python运算——以文本网络构建为例","slug":"Tensorflow加速Python运算——以文本网络构建为例","date":"2020-02-02T11:07:30.000Z","updated":"2020-02-08T15:56:03.609Z","comments":true,"path":"2020/02/02/Tensorflow加速Python运算——以文本网络构建为例/","link":"","permalink":"http://github.com/QinY-Stat/2020/02/02/Tensorflow%E5%8A%A0%E9%80%9FPython%E8%BF%90%E7%AE%97%E2%80%94%E2%80%94%E4%BB%A5%E6%96%87%E6%9C%AC%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA%E4%B8%BA%E4%BE%8B/","excerpt":"","text":"前言在先前的工作中，我自己曾经多次将复杂网络理论应用到NLP中，虽然获得了较好的效果，但其速度却非常慢。当时分析发现此类算法的时间复杂度为$O(mn^2)$，属于非常不理想的情况，因此没有再花时间对其进行优化。 然而，要对一个算法进行加速，能够优化其时间复杂度固然是最好不过，但使用更先进的计算方式也是一种很好的解决方案。在某一天幡然醒悟之后，我利用原程序、numpy.array以及tensorflow-gpu三种计算方案进行了一次时间对比实验，此后GPU在我头脑中的印象由那个只在深度学习中才使用的重武器真正转变为了一个优化利器。 文本网络概念所谓网络(Network)，又称为图(Graph)，是一种实体与关系的集合，其基本元素为节点(Node)与边(Edge)。 文本网络，即以网络形式表示的文本，最常用的文本网络是词共现网络(Co-Occurrence Network)。其节点即为文本中的一个个单词，这一点通常不会有变化。而边表示两个词在”共现”意义上的相似性，其定义方式比较多，主要取决于人们对于”共现”一词的理解。 一个基于网络的NLP算法能否取得理想的效果，与边的定义方式密切相关。在本实验中，我们认为两个词同时出现在一个句子中，即发生了共现。 文本表示我们利用词袋模型(Bag Of Words, BOW)来表示不同的句子。例如一个包含如下3个分好词的句子的文本：Code1231.我 爱 我 的 中国2.你 爱 你 的 美国3.我 爱 属于 你 和 我 的 地球其词频矩阵表示为以”地球”一词为例，其向量表示为[0, 0, 1] 构造网络得到了词的向量表示之后，我们用余弦相似度(similarity)来计算两个词的共现相似性。令$v_i$和$v_j$分别表示词$i$和词$j$的向量，则这两个词的共现相似性为：$$similarity({v_i},{v_j}) = \\frac{{{v_i} \\cdot {v_j}}}{{\\left\\| {{v_i}} \\right\\| \\cdot \\left\\| {{v_j}} \\right\\|}}$$当且仅当两个词的共现相似性不为0时，这两个词所对应的节点之间存在一条边。 三种计算方法我们要考察的，就是已知文本中的词集$\\bf{W}$与词频矩阵$\\bf{M}$的情况下，不同的计算方式计算文本网络的边集$\\bf{E}$并构建网络$\\bf{G}$的速度。代码的基本模式为Code123456789101112131415161718192021import networkx as nximport timedef get_edges(words, matrix): E &#x3D; [] ... return Eif __name__ &#x3D;&#x3D; &quot;__main__&quot;: # 给定词集W与词频矩阵M t0 &#x3D; time.time() G &#x3D; nx.Graph() V &#x3D; W G.add_nodes_from(V) E &#x3D; get_edges(W, M) G.add_weighted_edges_from(E) t1 &#x3D; time.time() print(&quot;Running Time :&quot;, t1 - t0)接下来的三种计算方法分别对应着三个不同的get_edges()函数。 实验需要将一个包含1307个句子、总共2347个词的文本表示为一个具有2347个节点与5324条边的网络。 for循环这种方法较为简单，是最容易写出来的方法，但是有得就有舍，这种方法的运算效率非常低下。其代码如下Code12345678910111213141516171819202122232425262728293031def similarity(vec1, vec2): dot_val &#x3D; 0. a_norm &#x3D; 0. b_norm &#x3D; 0. for a, b in zip(vec1, vec2): dot_val +&#x3D; a * b a_norm +&#x3D; a ** 2 b_norm +&#x3D; b ** 2 if a_norm * b_norm &#x3D;&#x3D; 0: cos &#x3D; 0 else: cos &#x3D; dot_val &#x2F; ((a_norm * b_norm) ** 0.5) return cosdef get_edges_for(words, mat): n &#x3D; len(words) words_dict &#x3D; &#123;i:j for i, j in enumerate(words)&#125; mat_dict &#x3D; &#123;i:j for i, j in enumerate(mat)&#125; E &#x3D; [] for i in range(n): word_i &#x3D; words_dict[i] vec_i &#x3D; mat_dict[i] for j in range(i, n): if i &#x3D;&#x3D; j: continue word_j &#x3D; words_dict[j] vec_j &#x3D; mat_dict[j] weight &#x3D; similarity(vec_i, vec_j) if weight: E.append((word_i, word_j, weight)) return E Numpy.Array利用Numpy.Array与Tensorflow进行加速的原理很简单，只是把上述两层for循环的内容转变为矩阵运算即可。Numpy.Array的代码如下Code1234567891011121314151617import numpy as npdef get_edges_array(words, mat): n &#x3D; len(words) words_dict &#x3D; &#123;i: j for i, j in enumerate(words)&#125; E &#x3D; [] numerator &#x3D; np.matmul(mat, mat.T) denominator &#x3D; np.outer(norm(mat, axis&#x3D;1), norm(mat, axis&#x3D;1)) weight_matrix &#x3D; numerator &#x2F; denominator for i in range(n): word_i &#x3D; words_dict[i] for j in range(i+1, n): word_j &#x3D; words_dict[j] weight &#x3D; weight_matrix[i, j] if weight: E.append((word_i, word_j, weight)) return E Tensorflow-gpuCode1234567891011121314151617181920212223import tensorflow as tfdef _get_edges_tf(words, mat): n &#x3D; len(words) words_dict &#x3D; &#123;i: j for i, j in enumerate(words)&#125; E &#x3D; [] tf_mat &#x3D; tf.constant(mat, dtype&#x3D;tf.float32) tf_mat_T &#x3D; tf.constant(mat.T, dtype&#x3D;tf.float32) numerator &#x3D; tf.matmul(tf_mat, tf_mat_T) norm_i &#x3D; tf.reshape(tf.norm(tf_mat, axis&#x3D;1), [n, 1]) norm_j &#x3D; tf.reshape(tf.norm(tf_mat_T, axis&#x3D;0), [1, n]) denominator &#x3D; tf.tensordot(norm_i, norm_j, axes&#x3D;1) tf_weight &#x3D; tf.divide(numerator, denominator) sess &#x3D; tf.Session() weight_matrix &#x3D; sess.run(tf_weight) for i in range(n): word_i &#x3D; words_dict[i] for j in range(i+1, n): word_j &#x3D; words_dict[j] weight &#x3D; weight_matrix[i, j] if weight: E.append((word_i, word_j, 0.5 + 0.5 * weight)) return E 实验结果实验环境如下表所示 名称 参数 CPU i5-8250U 内存 24GB 操作系统 Window 10 显卡(GPU) MX150(2G) 实验结果为 计算方法 时间(/s) for 5000+ Numpy.Array 65 Tensorflow-gpu 4.904","categories":[],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://github.com/QinY-Stat/tags/Tensorflow/"}]},{"title":"Fiedler向量与社区发现的那些事儿","slug":"Fiedler向量与社区发现的那些事儿","date":"2020-01-24T18:17:12.000Z","updated":"2020-01-25T08:25:54.640Z","comments":true,"path":"2020/01/25/Fiedler向量与社区发现的那些事儿/","link":"","permalink":"http://github.com/QinY-Stat/2020/01/25/Fiedler%E5%90%91%E9%87%8F%E4%B8%8E%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/","excerpt":"","text":"最早与Fiedler向量(Fiedler Vector，FV)的接触是师父给的一篇论文，该文作者提出了一种能够衡量删除某个节点或某条边对于网络的代数连通度的影响局部中心性，并提出了一个基于该中心性的社区发现算法。 事实上，FV并非第一次出现在社区发现算法中。在图论中具有重要地位的谱聚类算法(Spectral Clustering)将社区发现算法约简为在网络中发现最小割集(Minimum Cut)的方法。但为了避免划分出只包含1个节点的小社区，通常使用的两种割集准则为： 比例割集准则 (Ratio Cut) 规范化割集准则 (Normalized Cut) 在这两种准则下，将一个网络二分的最优方案是一句各节点的Fiedler值的符号(+、-)来对节点进行分类。下面给出推导过程以及存在的疑问(推导过程来自文献)： Fiedler Vector的有效性推导基于比例割集准则(Ratio Cut)的推导图$G=(V,E)$，令$\\bf{D}$为$G$的度矩阵，$\\bf{A}$为$G$的邻接矩阵，它们的元素分别为$${D_{ij}} = \\left\\{ \\begin{array}{l} {d_i}, \\qquad i = j\\\\ 0, \\qquad i \\ne j \\end{array} \\right. $$$${A_{ij}} = \\left\\{ \\begin{array}{l} 1, \\qquad (i,j) \\in E\\\\ 0, \\qquad (i,j) \\notin E \\end{array} \\right.$$其中${d_i} = \\sum\\limits_{j = 1}^n {{A_{ij}}}$表示节点$i$的度(Degree)。 $G$的Laplace矩阵$\\bf{L}=\\bf{D}-\\bf{A}$，对$\\forall \\bf{x} \\in {\\bf{R}^n}$，有$$\\begin{array}{l} {\\bf{x}^T}\\bf{L}\\bf{x} = {\\bf{x}^T}\\left( {\\bf{D} - \\bf{A}} \\right)\\bf{x} = {\\bf{x}^T}\\bf{D}\\bf{x} - {\\bf{x}^T}\\bf{A}\\bf{x} = \\sum\\limits_{i = 1}^n {{d_i}x_i^2} - \\sum\\limits_{i,j = 1}^n {{a_{ij}}{x_i}{x_j}} \\\\ {\\rm{ }} = \\frac{1}{2}\\left[ {\\sum\\limits_{i = 1}^n {{d_i}x_i^2} - 2\\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^n {{a_{ij}}{x_i}{x_j}} } + \\sum\\limits_{i = 1}^n {{d_i}x_i^2} } \\right]\\\\ {\\rm{ }} = \\frac{1}{2}\\left[ {\\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^n {{a_{ij}}x_i^2} } - 2\\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^n {{a_{ij}}{x_i}{x_j}} } + \\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^n {{a_{ij}}x_j^2} } } \\right]\\\\ {\\rm{ }} = \\frac{1}{2}\\sum\\limits_{i = 1}^n {\\sum\\limits_{j = 1}^n {{a_{ij}}{{\\left( {{x_i} - {x_j}} \\right)}^2}} } \\end{array}$$假设将$G$分割为$k$个不相交的子网络$C_1,C_2,…,C_k$，下文将这些子网络称为$k$个社区(Community)，将连接不同社区的节点的边称为$G$的桥(Bridge)，则连接任意两个社区$C_n$、$C_m$的桥的数量$W\\left( {{C_n},{C_m}} \\right) = \\sum\\limits_{i \\in {C_n}, \\in {C_m}} {{a_{ij}}}$，整个网络$G$的桥的数量$bridge\\left( {{C_1},{C_2},...,{C_k}} \\right) = \\frac{1}{2}\\sum\\limits_{i = 1}^k {W\\left( {{C_i},{\\bar C_i}} \\right)}$，其中$\\bar C_i$称为$C_i$的补图，即${C_i} \\cup {\\bar C_i}{\\rm{ = }}G$。此处只考虑$k=2$的情况，此时$bridge\\left( {{C_1},{C_2}} \\right) = W\\left( {{C_1},{C_2}} \\right)$。 接下来，引入指示向量(Indicator Vector)的概念。将$G$分割为$k$个社区时，${f_i} = \\left\\{ {{f_{i1}},{f_{i2}},...,{f_{ik}}} \\right\\}$是个$n$维向量，当$k=2$时，$f_i$的元素为$${f_{ij}} = \\left\\{ \\begin{array}{l} \\sqrt {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}}} {\\rm{ }},j \\in {C_1}\\\\ -\\sqrt {\\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}}} ,j \\in {C_2} \\end{array} \\right.$$则有$$\\begin{array}{* {20}{l}} {{f^T}Lf = \\frac{1}{2}\\sum\\limits_{i,j = 1}^n {{a_{ij}}{{\\left( {{f_i} - {f_j}} \\right)}^2}} }\\\\ {{\\rm{ }} = \\frac{1}{2}\\sum\\limits_{i \\in {C_1},j \\notin {C_1}} {{a_{ij}}{{\\left( {{f_i} - {f_j}} \\right)}^2}} + \\frac{1}{2}\\sum\\limits_{i \\in {C_2},j \\notin {C_2}} {{a_{ij}}{{\\left( {{f_i} - {f_j}} \\right)}^2}} }\\\\ {{\\rm{ }} = \\frac{1}{2}\\sum\\limits_{i \\in {C_1},j \\notin {C_1}} {{a_{ij}}{{\\left( {\\sqrt {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}}} + \\sqrt {\\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}}} } \\right)}^2}} + \\frac{1}{2}\\sum\\limits_{i \\in {C_2},j \\notin {C_2}} {{a_{ij}}{{\\left( { - \\sqrt {\\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}}} - \\sqrt {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}}} } \\right)}^2}} }\\\\ {{\\rm{ }} = \\sum\\limits_{i \\in {C_1},j \\in {C_2}} {{a_{ij}}\\left( {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}} + \\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}} + 2} \\right)} }\\\\ {{\\rm{ }} = \\sum\\limits_{i \\in {C_1},j \\in {C_2}} {{a_{ij}}\\left( {\\frac{{\\left| {{C_2}} \\right| + \\left| {{C_1}} \\right|}}{{\\left| {{C_1}} \\right|}} + \\frac{{\\left| {{C_1}} \\right| + \\left| {{C_2}} \\right|}}{{\\left| {{C_2}} \\right|}}} \\right)} }\\\\ {{\\rm{ }} = \\left( {\\left| {{C_1}} \\right| + \\left| {{C_2}} \\right|} \\right)\\left( {\\frac{1}{{\\left| {{C_1}} \\right|}} + \\frac{1}{{\\left| {{C_2}} \\right|}}} \\right)\\sum\\limits_{i \\in {C_1},j \\in {C_2}} {{a_{ij}}} }\\\\ {{\\rm{ }} = n\\left( {\\frac{1}{{\\left| {{C_1}} \\right|}} + \\frac{1}{{\\left| {{C_2}} \\right|}}} \\right)bridge\\left( {{C_1},{C_2}} \\right)} \\end{array}$$应当注意的是，$f$具有如下两个性质： $\\sum\\limits_{i = 1}^n {{f_i}} = \\sum\\limits_{i \\in {C_1}}^n {{f_i}} + \\sum\\limits_{i \\in {C_2}}^n {{f_i}} = \\sum\\limits_{i \\in {C_1}}^n {\\sqrt {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}}} } - \\sum\\limits_{i \\in {C_2}}^n {\\sqrt {\\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}}} } = \\left| {{C_1}} \\right| \\cdot \\sqrt {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}}} - \\left| {{C_2}} \\right| \\cdot \\sqrt {\\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}}} = 0$ $\\sum\\limits_{i = 1}^n {f_i^2} = \\sum\\limits_{i \\in {C_1}}^n {\\frac{{\\left| {{C_2}} \\right|}}{{\\left| {{C_1}} \\right|}}} + \\sum\\limits_{i \\in {C_2}}^n {\\frac{{\\left| {{C_1}} \\right|}}{{\\left| {{C_2}} \\right|}}} = \\left| {{C_2}} \\right| + \\left| {{C_1}} \\right| = n$ 即$f \\bot \\textbf{1}$与$\\sum\\limits_i {f_i^2} = n$(通常也会进行归一化，使得$\\sum\\limits_{i = 1}^n {f_i^2}=1$)。根据社区结构(Community Structure)的定义————“社区内节点联系紧密，社区间节点联系稀疏”，社区划分应当满足： 最优的社区结构应使得$bridge\\left( {{C_1},{C_2}} \\right)$尽可能小； 为了避免出现只包含一个节点的小社区，应当控制两个社区的规模的差异不宜过大，即使得$\\frac{1}{{\\left| {{C_1}} \\right|}} + \\frac{1}{{\\left| {{C_2}} \\right|}}$尽可能小。 综上所述，要找到最优的社区划分，即找到向量$f = \\mathop {\\arg \\min }\\limits_{f \\bot \\textbf{1},\\sum\\limits_i {f_i^2} = n} {f^T}\\textbf{L}f$。由于一个连通图$G$的$\\textbf{L}$是个半正定矩阵，$r\\left( \\textbf{L} \\right) = n - 1$，且$\\textbf{L1}=0$，其中$\\textbf{1} = \\left( {1,1,…,1} \\right)$不满足$f$的两个性质，所以$f$即为$\\textbf{L}$的第二小特征值对应的特征向量，即Fiedler向量。 基于规范化割集准则(Normalized Cut)的推导与Ratio Cut有些不同，Normalized Cut考虑到了节点的度，其指示函数为$${f_{ij}} = \\left\\{ \\begin{array}{l} \\sqrt {\\frac{{vol( {{C_2}})}}{{vol( {{C_1}}) }}} ,j \\in {C_1}\\\\ -\\sqrt {\\frac{{vol( {{C_1}}) }}{{vol( {{C_2}}) }}} ,j \\in {C_2} \\end{array} \\right.$$其中$vol(C_k)$表示社区$C_k$中所有节点的度之和，即$vol({C_k}) = \\sum\\limits_{i \\in {C_k}} $。Normalized Cut的目的是找到$n$维向量$$f = \\mathop {\\arg \\min }\\limits_{\\textbf{D}f \\bot \\textbf{1},{f^T}\\textbf{D}f = vol(V)} {f^T}\\textbf{L}f$$。","categories":[],"tags":[{"name":"复杂网络","slug":"复杂网络","permalink":"http://github.com/QinY-Stat/tags/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/"}]},{"title":"模块度(Modularity)发展历程","slug":"Modularity","date":"2020-01-21T18:29:07.000Z","updated":"2020-01-25T08:33:49.251Z","comments":true,"path":"2020/01/22/Modularity/","link":"","permalink":"http://github.com/QinY-Stat/2020/01/22/Modularity/","excerpt":"","text":"与小世界性[1]、无标度等基本统计特性相并列，Girvan&amp;Newman[2]在2001年发现的网络社区结构(Community Structure)是复杂网络最普遍和最重要的拓扑结构属性之一，具有相同社区节点相互连接密集、不同社区节点相互连接稀疏的特点，如图1所示。复杂网络中的社区发现方法目的在于揭示出复杂网络中真实存在的网络社区结构。 图1 社区结构为了度量实际网络中社区发现方法的好坏，Newman[3]于2004年提出了一个用于测度复杂网络中社区划分质量指标——模块度(Modularity)。在这之后，许多研究者以模块度函数作为目标函数，提出了一系列基于模块度最优的社区发现算法。 模块度的定义与发展1.1原始定义(Q1)1.1.1 迹(Trace)对于一个给定的网络$G=(V,E)$，其中$V$为网络的节点集，$E$为网络的边集。将$G$划分为$q$个社区，我们用一个$q\\times q$的对称矩阵$\\textbf{e}$来表示该划分，$\\textbf{e}$中的每个元素表示连接社区$i$与社区$j$的边在$G$的全部边中所占的比例，显然有$\\sum_{i,j}e_{ii}=1$。矩阵$\\textbf{e}$的迹$Tr(\\textbf{e})$表示连接社区内部节点的边的占比，一个好的社区划分应该有一个数值很高的迹。 但研究发现，单独使用迹$Tr(\\textbf{e})$在一些情况下无法很好地测度划分质量，例如这样一个平凡划分：将所有点放到同一个社区中。此时$Tr(\\boldsymbol{e})$=1为最大值，但该划分在绝大部分情况下显然不具有任何意义。 1.1.2 模块度(Q1)Newman[3]认为，考虑一个与$G$具有相同社区划分、相同边数但节点间随机连接的随机网络$G’$，若$G$具有社区结构，社区内部边占总边数的比例应该大于随机情况下的该比例的期望。实际社区内部边与期望的差越大，说明网络与随机网络的差异越大，即网络越具有社区结构。则模块度定义为 Q==Tr(\\boldsymbol{e})-\\left\\|\\boldsymbol{e}\\right\\|^{2} \\qquad \\qquad (1)理论上有$Q\\in[-1,1]$，但取值为$\\pm1$的情况是极为罕见的。$G$的一个社区划分的模块度为正数时，表明社区结构可能存在。在该定义下，平凡划分(将所有点放到同一个社区中)的模块度为$Q=0$。 1.2 考虑节点度的模块度(Q2)上述定义是简单且直接的，但存在一个问题：在构建随机网络$G’$时，没有考虑到原网络$G$的节点度。由于节点的度在一定程度上能够表示该节点被连接的概率，所以Newman[4]于2006年对模块度进行了重新定义。 考虑一个具有$n$个节点的无向无权网络，我们可以用它的邻接矩阵$\\textbf{A}$来表示该网络。$\\textbf{A}$是一个的$n\\times n$对称矩阵，通常来说 {A_{ij}} = \\left\\{ \\begin{array}{l} 1{\\rm{ }},(i,j) \\in E\\\\ 0{\\rm{ }},(i,j) \\notin E \\end{array} \\right. \\qquad \\qquad (2)一般地，我们约定$A_{ii}=0$。在允许多重边存在的网络中，$A_{ij}$允许取比1大的值。将网络划分为$q$个不重叠的社区，令$g_{i}$表示节点$i$所在的社区，则$\\boldsymbol{g}=(g_{1},…,g_{n})$是对网络的一个划分。对于一个特定的划分$\\boldsymbol{g}$，网络的社团内部边的数量为$\\frac{1}{2}\\sum\\limits_{i,j}{{A_{ij}}{\\delta_{{g_i}{g_j}}}}$，其中$\\delta_{ij}$是一个克罗内克函数(Kronecker delta): {\\delta_{ij}} = \\left\\{ \\begin{array}{l}1{\\rm{ }},(i,j) \\in E\\\\0{\\rm{ }},(i,j) \\notin E \\end{array} \\right. \\qquad \\qquad (3)对于随机网络$G’$，保持原网络$G$的边数$m$与各节点的度不变，将这$m$条边随机地重新定位到任意两个节点间。随机化后，令$P_{ij}$表示节点$i$与$j$相连接的概率，则社区内部边数的期望值为$\\frac{1}{2}\\sum\\limits_{i,j} {{P_{ij}}{\\delta_{{g_i}{g_j}}}}$。故重新定义的模块度为：$$\\begin{array}{l} Q = \\frac{1}{m}\\left( {\\frac{1}{2}\\sum\\limits_{i,j} {{A_{ij}}{\\delta_{{g_i}{g_j}}}} - \\frac{1}{2}\\sum\\limits_{i,j} {{P_{ij}}{\\delta_{{g_i}{g_j}}}} } \\right)\\\\{\\rm{ }} = \\frac{1}{{2m}}\\sum\\limits_{i,j} {({A_{ij}} - {P_{ij}}){\\delta_{{g_i}{g_j}}}} \\\\ Q \\in [ - 0.5,1] \\end{array} \\qquad \\qquad (4)$$对于一个特定的网络，模块度的大小与$m$不相关，仅与社区划分有关。这一定义使用边数的比例而不是边数的绝对值，这使得不同规模的网络划分的模块度可以进行比较。注意到，此时平凡划分的模块度为：$$Q=\\frac{1}{{2m}}\\sum\\limits_{i,j} {({A_{ij}} - {P_{ij}}) = 0} \\qquad \\qquad (5)$$因为在随机化中，网络中边的数量被视为常数，即$\\sum\\nolimits_{ij}{{P_{ij}}}=\\sum\\nolimits_{ij} {{A_{ij}} = 2m}$。现在需要确定如何计算$P_{ij}$。这个值依赖于我们用来做随机化的方案。一个简单的方案是，对于任意两个节点$i$和$j$，令它们具有相同的连接概率，即对于给定边数$m$、节点数$n$的网络，有：$$P_{ij}=\\frac{m}{{C_{n}^{2}}} \\qquad \\qquad (6)$$理论上说，这里$P_{ij}$应该是节点$i$与节点$j$之间的边数的期望而不是$i$与$j$相连接的概率，但由于$m&lt;&lt;{C_{n}^{2}}$，故期望与概率很接近。然而实际上，节点间的连接很大程度上依赖于节点的度，但如(6)式表示的$P_{ij}$没有考虑到节点的度。因此，我们需要一个能够保持节点度数的带约束随机化方案。这里，Newman使用了configuration model产生随机图集合[5],[6]。随机化后，两个节点之间的连接概率为$$P_{ij}=\\frac{k_{i}k_{j}}{{2m}}\\qquad \\qquad (7)$$其中${k_i} = \\sum\\nolimits_j {{A_{ij}}}$是节点i的度。至此，我们得到了重新定义的模块度：$$Q = \\frac{1}{2}\\sum\\limits_{i,j} {\\left( {{A_{ij}} - \\frac{{{k_i}{k_j}}}{{2m}}} \\right){\\delta_{{g_i}{g_j}}}} \\qquad \\qquad (8)$$Newman指出，在实践中，好的划分的$Q$值应当取值在0.3~0.7之间，更高的数值是很罕见的。这一重新定义的模块度也是目前使用范围最广的模块度。 1.3 分辨率限制与广义模块度(Q3)1.3.1 分辨率限制(Resolution Limit)根据Fortunato&amp;Barthelemy[7]的研究成果，在对(8)式进行最优化来寻找社团划分时有一个缺点：无法找出那些实际具有许多小社团的网络中的社团划分。特别地，当网络中的社团数大于$\\sqrt{2m}$时，通过使(8)最大化找到的社团划分与正确划分可能不符——基于(8)的模块度最优化方法寻找到的社区划分倾向于将一个个小社区合并成更大的社区，这样使得基于模块度最优的方法不能发现网络中较小的社区。 1.3.2 广义模块度{Generalized Modularity)为了解决上述问题，Arenas[8]在2008年基于Reichardt&amp;Bornholdt[9]在2006年对启发式概念与汉密尔顿函数(Hamiltonian)的推导与讨论，提出了一个广义模块度的概念：$$Q\\left( \\gamma \\right) = \\frac{1}{2}\\sum\\limits_{i,j} {\\left( {{A_{ij}} - \\gamma \\frac{{{k_i}{k_j}}}{{2m}}} \\right){\\delta_{{g_i}{g_j}}}} \\qquad \\qquad (9)$$显然，(8)定义的模块度为(9)的一个特殊情况($\\gamma=1$)。 1.4 对于Q3中参数的一些补充说明Q3与Q2的区别在于参数$\\gamma$，称为分辨率参数(resolution parameter)。这使得Q3优化法比Q2优化法具有更高的灵活性，能够发现更多结构下的网络社区划分。但即便是这样，模块度优化法也存在一定的问题。 Lancichinetti&amp;Fortunato[10]在2012年证实了多分辨率模块度(Q3)存在着一个问题：当取较小的值时，倾向于将小的社团合并为大社团；当大时，倾向于将大社团分裂在社团大小分布不一的网络中。而我们不可能同时消除这两种偏差，Q3也无法发现正确的社区结构。 Jeub、Sporns&amp;Fortunato[14]等人指出，最佳划分应当对应于分辨率参数取正确值的位置。但是在缺乏关于网络的先验知识的情况下，预先确定是很困难的，尽管Newman[11]于2015年证明了基于Q2优化的社区发现算法在特定情况下与极大似然估计方法等价，将模块度优化法与下列2种模型联系了起来，并提出了一种可能的计算方法。 Stochastic Block Model (SBM) Planted Partition Model (PPM) Newman,20152.1 Stochastic Block Model (SBM)2.1.1 模型及参数定义 参数 参数说明 $m$ 网络中的边数 $n$ 网络中的节点数 $q$ 网络中划分的社区数 $\\omega_{rs}$ 社区$r$与社区$s$的连接概率 $\\bf{g}$ 一个$n$维向量(社区划分) $\\bf{A}$ 一个$n\\times n$的对称矩阵(邻接矩阵) $\\bf{\\Omega}$ 一个$q\\times q$的对称矩阵(社团邻接矩阵) SBM是一种生成随机图的模型。给定网络的先验信息$m$、$n$、$q$以及$\\bf{\\Omega}$和$\\bf{g}$，将$m$条边随机分配到各节点之间形成一个随机网络。 2.1.2 SBM的一些说明Newman指出，随机块模型生成随机网络的过程中，连接任意两个节点$i$和$j$的边的数量通常不是随机的，而是服从以为$\\omega_{rs}$参数的Poisson分布，其中$r$、$s$分别为节点i和j所在的社团。至此，可以得出随机块模型假设网络$\\textbf{A}$的连接的生成过程为： 将网络中每个节点$i$以概率$\\theta_{k}$分配到第$k$个社区，$\\theta$为一个$q$维向量，节点$i$的社区变量用$Z_i$表示: {Z_i} \\sim Mult\\left( \\theta \\right) 每对节点产生连接$A_{ij}$的概率服从Poisson分布，即${A_{ij}} \\sim Poisson\\left( {{\\omega_{{g_i}{g_j}}}} \\right)$。 因此在理论上，在生成的随机网络中，一对节点之间可能存在多条边，并且允许存在节点自己与自己相连接的情况。与其他情况不太一样的是，节点与自身相连边的数量服从以$\\frac{1}{2}\\omega_{rr}$为参数的Poisson分布。(实际上，还有另一个Bernoulli分布版本的随机块模型，也适用于往下得出的结论，但Newman在文章中仅给出了Poisson分布版本) 上述随机网络的假设在某些情况下是适用的，例如论文引用网络与万维网超链接网络：一篇论文可在另一篇论文中被重复引用，一个网页在另一个网页中也可能被重复引用。在其他大多数情况下，上述随机网络显然不太符合现实。但由于大部分的现实网络是非常稀疏的，这意味着边连接概率$\\omega_{rs}$非常小。因此在这种情况下，网络中多重边与自连边的密度很小，通常可以忽略不计。 2.2 基于SBM的极大似然估计法上述SBM是基于先验信息生成一个随机网络。而社区发现算法则与其相反，是根据给定的观测网络来推断社区划分。具体地，这里希望求出的参数是：边的概率$\\omega_{rs}$和社团成员$g_i$。 假定一个邻接矩阵为$\\bf{A}$的观测网络由一个参数为$\\bf{\\Omega}$和$\\bf{g}$的Poisson形式的SBM生成，则观测网络由该模型生成的概率为：$$P\\left( {\\bf{A}|\\bf{\\Omega} ,\\bf{g}} \\right) = \\prod\\limits_i {P\\left( {{A_{ij}}|\\Omega ,g} \\right)} = \\prod\\limits_i {\\frac{{{{\\left( {\\frac{1}{2}{\\omega_{{g_i}{g_j}}}} \\right)}^{\\frac{{{A_{ii}}}}{2}}}}}{{\\left( {\\frac{1}{2}{A_{ii}}} \\right)!}}{e^{ - \\frac{{{\\omega_{{g_i}{g_j}}}}}{2}}}\\prod\\limits_{i < j} {\\frac{{\\omega_{{g_i}{g_j}}^{{A_{ij}}}}}{{{A_{ij}}!}}{e^{ - {\\omega_{{g_i}{g_j}}}}}} } \\qquad (10)$$这里约定$\\bf{A}$的对角线元素$A_{ii}=2$，而不是通常的取值1。这一概率的最大值所对应的$\\bf{\\Omega}$和$\\textbf{g}$就是最可能产生观测网络的参数值。 对(10)式取对数形式得:$$\\log P\\left( {\\textbf{A}|\\bf{\\Omega} ,\\textbf{g}} \\right) = \\sum\\limits_i {\\left[ {\\frac{1}{2}{A_{ii}}\\log \\left( {\\frac{1}{2}{\\omega_{{g_i}{g_i}}}} \\right) - \\frac{1}{2}{\\omega_{{g_i}{g_i}}} - \\log \\left[ {\\left( {\\frac{1}{2}{A_{ii}}} \\right)!} \\right]} \\right]} + \\sum\\limits_{i < j} {\\left( {{A_{ij}}\\log {\\omega_{{g_i}{g_j}}} - {\\omega_{{g_i}{g_j}}} - \\log \\left( {{A_{ij}}!} \\right)} \\right)} \\qquad (11)$$其中$\\frac{1}{2}{A_{ii}}\\log \\frac{1}{2}$、$\\log \\left( {\\left( {{\\textstyle{1 \\over 2}}{A_{ij}}} \\right)!} \\right)$、$\\log \\left( {{A_{ij}}!} \\right)$与两个参数独立，不影响该概率的最大值，将其视为常数并忽略掉，则上式简化为：$$\\log P\\left( {{\\bf{A}}|{\\bf{\\Omega }},{\\bf{g}}} \\right) = \\frac{1}{2}\\sum\\limits_{i,j} {\\left( {{A_{ij}}\\log {\\omega_{{g_i}{g_j}}} - {\\omega_{{g_i}{g_j}}}} \\right)} \\qquad (12)$$ 2.3 Degree-Corrected Block Model (DCBM)如同1.1节中Q1一样，SBM没有考虑到节点度的影响。这导致该模型(无论是Bernoulli形式还是Poisson形式)生成了一个具有Poisson(或Bernoulli)度分布的网络，这与经验网络中的广泛分布非常不同。 为了解决这一问题，Karrer与Newman[12]在2011年提出了DCBM，将连接节点$i$与$j$的边数的期望值由$\\omega_{rs}$改为了$\\frac{k_{i}k_{j}}\\omega_{rs}$，其中自连边的期望保持不变。与1.2节中一样，$\\frac{k_{i}k_{j}}{{2m}}$是随机化时节点$i$与$j$的连接概率，$\\omega_{rs}$代表两个社团之间相连接的概率。 遵循与2.2中相同的推导思路，忽略那些对极大似然估计值的位置没有影响的因素，得到$$\\log P\\left( {{\\bf{A}}|{\\bf{\\Omega }},{\\bf{g}}} \\right) = \\frac{1}{2}\\sum\\limits_{i,j} {\\left( {{A_{ij}}\\log {\\omega_{{g_i}{g_j}}} - \\frac{{{k_i}{k_j}}}{{2m}}{\\omega_{{g_i}{g_j}}}} \\right)} \\qquad \\qquad (13)$$至此，社区发现算法转化为:寻找合适的$\\bf{\\Omega}$与$\\bf{g}$使(13)最大化，以寻找出与观测网络最匹配的社区划分。 2.4 Planted Partition Model (PPM)PPM是SBM的一个特例，其描述社团结构的参数仅取两个值：$${\\omega_{rs}} = \\left\\{ \\begin{array}{l} {\\omega_{in}}{\\rm{ }} \\quad if{\\rm{ }} \\quad r = s\\\\ {\\omega_{out}}{\\rm{ }}\\quad if{\\rm{ }} \\quad r \\ne s \\end{array} \\right. \\qquad \\qquad (14)$$这意味着它假设各个社团拥有相同的社团内部、社团间连接概率。根据(14)可得：$${\\omega_{rs}} = \\left( {{\\omega_{in}} - {\\omega_{out}}} \\right){\\delta_{rs}} + {\\omega_{out}} \\qquad \\qquad (15)$$$$\\log {\\omega_{rs}} = \\left( {\\log {\\omega_{in}} - \\log {\\omega_{out}}} \\right){\\delta_{rs}} + \\log {\\omega_{out}} \\qquad \\qquad (16)$$将(15)、(16)代入(13)得$$\\begin{array}{l} \\log P\\left( {\\bf{A}|\\bf{\\Omega} ,\\bf{g}} \\right) = \\frac{1}{2}\\sum\\limits_{i,j} {{A_{ij}}\\left[ {{\\delta_{{g_i}{g_j}}}\\log \\frac{{{\\omega_{in}}}}{{{\\omega_{out}}}} + \\log {\\omega_{out}}} \\right] - } \\frac{1}{2}\\sum\\limits_{i,j} {\\frac{{{k_i}{k_j}}}{{2m}}\\left[ {\\left( {{\\omega_{in}} - {\\omega_{out}}} \\right){\\delta_{{g_i}{g_j}}} + {\\omega_{out}}} \\right]} \\\\ {\\rm{ }} = \\frac{1}{2}\\sum\\limits_{i,j} {\\left[ {{A_{ij}}\\log \\frac{{{\\omega_{in}}}}{{{\\omega_{out}}}} - \\left( {{\\omega_{in}} - {\\omega_{out}}} \\right)\\frac{{{k_i}{k_j}}}{{2m}}} \\right]{\\delta_{{g_i}{g_j}}}} + \\frac{1}{2}\\sum\\limits_{i,j} {\\left[ {{A_{ij}}\\log {\\omega_{out}} - \\frac{{{k_i}{k_j}}}{{2m}}{\\omega_{out}}} \\right]} \\\\ {\\rm{ }} = \\frac{1}{2}\\log \\frac{{{\\omega_{in}}}}{{{\\omega_{out}}}}\\sum\\limits_{i,j} {\\left[ {{A_{ij}} - \\frac{{{\\omega_{in}} - {\\omega_{out}}}}{{\\log {\\omega_{in}} - \\log {\\omega_{out}}}} \\cdot \\frac{{{k_i}{k_j}}}{{2m}}} \\right]{\\delta_{{g_i}{g_j}}}} + m\\left( {\\log {\\omega_{out}} - {\\omega_{out}}} \\right)\\\\ {\\rm{ }} = \\text{B}\\frac{1}{{2m}}\\sum\\limits_{i,j} {\\left( {{A_{ij}} - \\gamma \\frac{{{k_i}{k_j}}}{{2m}}} \\right)} {\\delta_{{g_i}{g_j}}} + C \\end{array} \\qquad \\qquad (17)$$其中，$\\text{B}$与$C$是仅与$\\delta_{in}$和$\\delta_{out}$有关、与$\\bf{g}$无关的常数，且$$\\gamma = \\frac{{{\\omega_{in}} - {\\omega_{out}}}}{{\\log {\\omega_{in}} - \\log {\\omega_{out}}}} \\qquad \\qquad (18)$$为了进行社区发现，现在要将(17)这个关于社区分配$\\bf{g}$与参数$\\omega_{in}$、$\\omega_{out}$的函数最大化。假设我们已知正确参数$\\omega_{in}$和$\\omega_{out}$，那么当且仅当获得正确的$\\bf{g}$时，该值最大。 注意到，式(17)除开两个常数$\\text{B}$与$C$外，与(9)式并没有什么不同。由此可知，式(17)与式(9)在相同位置上取到最大值，即已知参数$\\omega_{in}$和$\\omega_{out}$的PPM的极大似然估计法与以式(18)为参数的广义模块度最优法等价。 2.5 解析参数的值要求解正确的$\\gamma$，只需求出$\\omega_{in}$与$\\omega_{out}$，这里使用迭代方法来进行求解。其思想为，先设定一个初始的$\\gamma$值，利用模块度最优法获取一个社区划分，对该划分，社团内部边$m_{in}$的期望为：$${m_{in}} = \\frac{1}{2}\\sum\\limits_r {\\sum\\limits_{i,j} {\\frac{{{k_i}{k_j}}}{{2m}}{\\omega_{rr}}{\\delta_{{g_i},r}}{\\delta_{{g_j},r}}} } = \\frac{{{\\omega_{in}}}}{{4m}}\\sum\\limits_r {\\kappa_r^2} \\qquad \\qquad (19)$$其中${\\kappa_r} = \\sum\\nolimits_i {{k_i}{\\delta_{{g_i},r}}} $表示社团r中所有节点的度的和。由式(19)可得$${\\omega_{in}} = \\frac{{2{m_{in}}}}{{\\frac{1}{{2m}}\\sum\\nolimits_r {\\kappa_r^2} }} \\qquad \\qquad (20)$$同理可得$${\\omega_{out}} = \\frac{{2{m_{out}}}}{{\\frac{1}{{2m}}\\sum\\nolimits_{r \\ne s} {{\\kappa_r}{\\kappa_s}} }} = \\frac{{2m - 2{m_{in}}}}{{2m - \\frac{1}{{2m}}\\sum\\nolimits_r {\\kappa_r^2} }} \\qquad \\qquad (21)$$实际计算时，使用所有社团内部边的观测值来表示$m_{in}$。综上，总结求解解析参数$\\gamma$的步骤如下： 给$\\gamma$设定一个初始值； 接着，根据给定的网络$G$、$\\gamma$、社团数目$q$，根据模块度最优法寻找社团划分； 基于该社团划分，计算$\\gamma$，获得新的模块度函数； 重复步骤2和步骤3，直到社团结构不再改变，获得正确$\\gamma$的与社团划分。 2.6 一些补充说明模块度最优法与极大似然估计法的等价说明了： 随机块模型统一假设网络具有Poisson度分布，使得拟合网络与真实网络的近似性很差。相比之下，度修正模型适合任何度分布网络。如Bickel和Chen[13]的研究成果显示，在适当的条件下，模块度最优得到的社区结构是正确且无偏的。 $B = m\\log \\frac{{{\\omega_{in}}}}{{{\\omega_{out}}}}$的符号取决于$\\omega_{in}$和$\\omega_{out}$的大小。当${\\omega_{in}} &gt; {\\omega_{out}}$时，原结论成立；当${\\omega_{in}} &lt; {\\omega_{out}}$时，极大似然估计与模块度最小化法等价。 Newman假设$\\omega_{rs}$仅取两个值：$\\omega_{in}$、$\\omega_{out}$，这意味着任意社团内部、任意两个社团之间的连接程度相同，这与现实不符。故该指标虽然在理论上有所依托，但实际运用时还存在较大困难。 关于Newman提出的$\\gamma$计算方案的疑惑：在2.4节中，Newman令$B = m\\log \\frac{{{\\omega_{in}}}}{{{\\omega_{out}}}}$与$C = m\\left( {\\log {\\omega_{out}} - {\\omega_{out}}} \\right)$时，作出的假设是“$\\text{B}$与$C$是仅与$\\omega_{in}$和$\\omega_{out}$有关，与$\\bf{g}$无关的常数”。但在2.5节计算参数时，$\\gamma$的取值与$m_{in}$、$\\sum\\nolimits_r {\\kappa_r^2}$两个数有关。其中$\\kappa_{r}$代表社团$r$内部节点的度数之和。显然，不同的社团划分方案(即$\\bf{g}$)对应着不同的$\\sum\\nolimits_r {\\kappa_r^2}$值，与2.4中作出的结论矛盾。 参考文献 Watts DJ, Strogatz SH. Collective dynamics of ‘small-world’ networks. Nature, 1998,393(6638):440-442. Girvan M, Newman M E J. Community structure in social and biological networks. Proc. of the National Academy of Science, 2002,99(12):7821-7826. Newman M E J, Girvan M. Finding and evaluating community structure in networks. Phys. Rev. E, 2004,69(2):026113. Newman M E J. Modularity and community structure in networks. Proc. of the National Academy of Science, 2006,103(23):8577-8582. Molloy M, Reed B. A critical point for random graphs with a given degree sequence. Random Structures and Algorithms,1995,6:161-179. Newman M E J, Strogatz SH, Watts DJ. Random graphs with arbitrary degree distributions and their applications. Phys. Rev. E,2001,64:026118. Fortunato S, Barthelemy M. Resolution limit in community detection. Proc. of the National Academy of Science,2007,104(1):36-41. Arenas A, Fernandez A, Gomez S. Analysis of the structure of complex networks at different resolution levels. New Journal Of Physics,2008,10:053039. Reichardt J, Bornholdt S. Statistical mechanics of community detection. Phys. Rev. E,2006,74(1):016110. Lancichinetti A, Fortunato S. Limits of modularity maximization in community detection. Phys. Rev. E,2011,84(6):066122. Newman M E J. Equivalence between modularity optimization and maximum likelihood methods for community detection. Phys. Rev. E,2016,94(5):052315. Karrer B, Newman M E J. Stochastic blockmodels and community structure in networks. Phys. Rev. E,2011,83:016107. Bickel PJ, Chen A. A nonparametric view of network models and Newman-Girvan and other modularities. Proc. of the National Academy of Science,2009,106:21068-21073. Jeub L G S, Sporns O, Fortunato S. Multiresolution Consensus Clustering in Networks. Scientific Reports,2018,8:3259.","categories":[],"tags":[{"name":"复杂网络","slug":"复杂网络","permalink":"http://github.com/QinY-Stat/tags/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/"}]}]}